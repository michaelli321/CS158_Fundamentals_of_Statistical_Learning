{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDS/ACM/CS 158: Fundamentals of Statistical Learning\n",
    "### PS1, Problem 4: K-NN and Linear Regression for Classification\n",
    "> Name: Li, Michael\n",
    " \n",
    "> Email address: mlli@caltech.edu\n",
    "\n",
    "Notes:\n",
    "Please use python 3.6\n",
    "\n",
    "You are required to properly comment and organize your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Helper functions (add/remove part label according to the specific question requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    filename - filename to open and load\n",
    "    \n",
    "    Returns file as matrix where last column is\n",
    "    y_i and columns up to last one is x_i\n",
    "    \"\"\"\n",
    "    res = np.loadtxt(open(filename, \"rb\"), delimiter=\",\", skiprows=1)\n",
    "    return res\n",
    "\n",
    "def linreg_regression(D, X):\n",
    "    \"\"\"\n",
    "    D - training data consisting of pairs of p-dimensional vectors and output\n",
    "    X - a column p-vector that represents a new input\n",
    "    \n",
    "    Returns the linear regression of X using D\n",
    "    \"\"\"\n",
    "\n",
    "    x = D[:,:-1]\n",
    "    y = D[:,-1]\n",
    "    \n",
    "    # add bias term to training data\n",
    "    bias = np.matlib.repmat(1, len(x), 1)\n",
    "    x = np.concatenate((bias, x), axis=1)\n",
    "    \n",
    "    # calculate beta\n",
    "    intermediate = np.matmul(x.transpose(), x)\n",
    "    inverse_intermediate = np.linalg.inv(np.array(intermediate))\n",
    "    pseudo_x = np.matmul(inverse_intermediate, x.transpose())\n",
    "    \n",
    "    beta = np.matmul(pseudo_x, y)\n",
    "    \n",
    "    # apply beta weight to X\n",
    "    return np.matmul(np.insert(X, 0, 1), beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>Part A</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classification(K, D, X):\n",
    "    \"\"\"\n",
    "    K - number of neighbors\n",
    "    D - training data consisting of pairs of p-dimensional vectors and outputs\n",
    "    X - a column p-vector that represents a new input\n",
    "    \n",
    "    Returns the K-NN classification of X using D\n",
    "    \"\"\"\n",
    "    \n",
    "    train_x = D[:,:-1]\n",
    "    train_y = D[:,-1]\n",
    "    \n",
    "    # find distances to X and sort points in D by that\n",
    "    dists = np.sqrt(np.sum((train_x - np.matlib.repmat(X, len(train_x), 1))**2, axis=1))\n",
    "    inds = dists.argsort()\n",
    "    \n",
    "    # count the occurences of a class within first K and choose the most common one\n",
    "    nearest_neighbors = train_y[inds][:K].tolist()\n",
    "    return max(nearest_neighbors, key=nearest_neighbors.count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>Part B</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_classification(D, X):\n",
    "    \"\"\"\n",
    "    D - training data consisting of pairs of p-dimensional vectors and output\n",
    "    X - a column p-vector that represents a new input\n",
    "    \n",
    "    Returns the linear regression classification of X using D\n",
    "    \"\"\"\n",
    "    \n",
    "    if linreg_regression(D, X) < .5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>Part C</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would expect linear regression classification to work better on dataset 3 since it looks mostly linearly separable with some noise. I would expect k-NN with K=1 to work better on dataset 4 since it does not appear to be linearly separable. It would be better in this case to just take the closest point in the training set since it doesn't seem like any line could split the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_vs_linear_reg(train_filename, test_filename, dataset):\n",
    "    \"\"\"\n",
    "    train_filename - filename of training data to load\n",
    "    test_filename - filename of test data to load\n",
    "    dataset - number of dataset\n",
    "    \n",
    "    Prints Results for KNN vs LinReg\n",
    "    \"\"\"\n",
    "    K = 1\n",
    "    training_data = load_data(train_filename)\n",
    "    test_data = load_data(test_filename)\n",
    "\n",
    "    test_x = test_data[:,:-1]\n",
    "    test_y = test_data[:,-1]\n",
    "    \n",
    "    # run KNN and Linear Regression on all points in test dataset\n",
    "    knn = [knn_classification(K, training_data, test_x[i]) for i in range(len(test_x))]\n",
    "    lr = [linreg_classification(training_data, test_x[i]) for i in range(len(test_x))]\n",
    "    \n",
    "    # compute the zero-one loss of both models\n",
    "    Err_knn = np.mean(np.not_equal(test_y, knn))\n",
    "    Err_lr = np.mean(np.not_equal(test_y, lr))\n",
    "    R = Err_knn / Err_lr  \n",
    "\n",
    "    print('For dataset {}\\n Err_knn is {:1.4f} \\n Err_lr is {:1.4f} \\n R = {:1.4f}'.format(dataset, Err_knn, Err_lr, R))\n",
    "    if R > 1:\n",
    "        print(' Linear regression is better.')\n",
    "    else:\n",
    "        print(' k-NN is better.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dataset 3\n",
      " Err_knn is 0.3410 \n",
      " Err_lr is 0.2470 \n",
      " R = 1.3806\n",
      " Linear regression is better.\n",
      "\n",
      "For dataset 4\n",
      " Err_knn is 0.2040 \n",
      " Err_lr is 0.2960 \n",
      " R = 0.6892\n",
      " k-NN is better.\n"
     ]
    }
   ],
   "source": [
    "knn_vs_linear_reg('dataset3_train.csv', 'dataset3_test.csv', 3)\n",
    "print()\n",
    "knn_vs_linear_reg('dataset4_train.csv', 'dataset4_test.csv', 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
